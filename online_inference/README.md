## Описание работы 
В данном разделе предпринята попытка выполнить второе домашнее задание по курсу ML в Production.
В ходе работы удалось починить проблему с тестами, о которой упоминалось в коммите первого ДЗ, однако вскрылись некоторые 
ошибки в построении архитектуры сервиса, в частности, оказалось, что не верно было понятно требование про "ручной" трансформер. 
Из-за этого на втором этапе возникли проблемы с pipeline, потому местами получилось немного криво, к сожалению. Но опыт получены, выводы сделаны. 

Чтобы собрать docker файл, необходимо выполнить следующую команду:

```docker build -t online_inference:v1 online_inference/.```

После чего его можно будет запустить с помощью команды:

```docker run -p 80:80 online_inference:v1 ```

После чего если запустить локально скрипт, то можно в консоли увидеть результат обращения. Запустить скрипт можно с помощью команды:

```  python3 request_script.py ```

В файле ``test_predict.py`` подгтовлен простейший тест, который проверят ответ на запрос. 

### Самопроверка

0) ветку назовите homework2, положите код в папку online_inference – Сделано

1) Оберните inference вашей модели в rest сервис(вы можете использовать как FastAPI, так и flask, другие желательно не использовать, дабы не плодить излишнего разнообразия для проверяющих), должен быть endpoint /predict (3 балла) – Сделано

2) Напишите тест для /predict  (3 балла) (https://fastapi.tiangolo.com/tutorial/testing/, https://flask.palletsprojects.com/en/1.1.x/testing/) – В целом, скорее сделано

3) Напишите скрипт, который будет делать запросы к вашему сервису -- 2 балла – Сделано

4) Сделайте валидацию входных данных (например, порядок колонок не совпадает с трейном, типы не те и пр, в рамках вашей фантазии)  (вы можете сохранить вместе с моделью доп информацию, о структуре входных данных, если это нужно) -- 3 доп балла
https://fastapi.tiangolo.com/tutorial/handling-errors/ -- возращайте 400, в случае, если валидация не пройдена – А вот тут не получилось

5) Напишите dockerfile, соберите на его основе образ и запустите локально контейнер(docker build, docker run), внутри контейнера должен запускать сервис, написанный в предущем пункте, закоммитьте его, напишите в readme корректную команду сборки (4 балл) – Сделано

6) Оптимизируйте размер docker image (3 доп балла) (опишите в readme.md что вы предприняли для сокращения размера и каких результатов удалось добиться)  -- https://docs.docker.com/develop/develop-images/dockerfile_best-practices/ – Тут, к сожалению, не успел 

7) опубликуйте образ в https://hub.docker.com/, используя docker push (вам потребуется зарегистрироваться) (2 балла) – собранный docker выложен в репозитории:
 https://hub.docker.com/repository/docker/fahrengeit/ml_in_prod с тегом `hw2`


8) напишите в readme корректные команды docker pull/run, которые должны привести к тому, что локально поднимется на inference ваша модель (1 балл) – Этот пункт я не очень понял, что нужно описать 
9) Убедитесь, что вы можете протыкать его скриптом из пункта 3 – Тут все работает. 

Если сложить баллы по указанным пунктам, то получается 14 баллов + 1 за самопроверку, итого 15. 